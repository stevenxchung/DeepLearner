# DeepLearner üß†

> **End-to-end multimedia-to-text AI companion ‚Äî runs üíØ locally üëÄ.**  
> Convert **video ‚Üí audio ‚Üí text** for analysis, retrieval, and language model integration.

[![Python](https://img.shields.io/badge/python-3.12%2B-orange.svg)](https://www.python.org/)
[![TypeScript](https://img.shields.io/badge/made%20with-TypeScript-blue)](https://www.typescriptlang.org/)
[![Runs Locally](https://img.shields.io/badge/runs-locally-green)]()

---

## üöÄ Features

- üé• **Video to Audio** conversion via `FFmpeg` or `yt-dlp`
- üîâ **Audio to Text** transcription with `faster-whisper` (GPU-accelerated)
- ü§ñ **LLM Integration** with local models via [Ollama](https://ollama.com)
- üß∞ Built with [`uv`](https://docs.astral.sh/uv/getting-started/installation/) and [`Bun`](https://bun.sh), ultra-fast Python and JavaScript package managers
- üíª Runs completely offline for maximum privacy and minimal cost (just your ‚ö° bill üëÄ)

---

## üß± Requirements

The following software needs to be installed on your local machine before running.

### üì¶ Package Managers

For speed and efficiency we highly recommend the following:

- [**Bun**](https://bun.sh) ‚Äì Fast JavaScript runtime & package manager
- [**uv**](https://docs.astral.sh/uv/getting-started/installation/) ‚Äì Blazing fast Python environment manager (written in Rust)

### ü§ñ Local LLMs

- [**Ollama**](https://ollama.com/download) ‚Äì A streamlined, open-source platform for running and managing LLMs on your local machine. It simplifies downloading, setting up, and interacting with open-source models

> ‚ÑπÔ∏è Make sure Ollama is running in the background for LLM-based workflows.

### üéûÔ∏è Video to Audio Tools

- [**FFmpeg**](https://github.com/FFmpeg/FFmpeg) ‚Äì A powerful multimedia toolkit for handling audio, video, subtitles, and metadata
- [**yt-dlp**](https://github.com/yt-dlp/yt-dlp) ‚Äì A feature-rich CLI tool for downloading videos and audio from thousands of websites (a modern fork of youtube-dl)

### üéôÔ∏è Audio to Text (Transcription)

To enable GPU-accelerated transcription with [`faster-whisper`](https://github.com/SYSTRAN/faster-whisper):

- NVIDIA GPU with sufficient VRAM for your chosen model
- NVIDIA GPU driver (version depends on your CUDA setup)
- [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) (typically version 11+)
- [cuDNN](https://developer.nvidia.com/cudnn-downloads) (sometimes bundled with CUDA)

To ensure PyTorch is installed with CUDA support:

```bash
# ./transcription-api
uv pip install torch --index-url https://download.pytorch.org/whl/cu128 && uv sync
```
